# -----------------------------------------------------------------------------
# docker-compose.yaml — CoSMIC Coaching Writer Deployment Stack
# -----------------------------------------------------------------------------
# Purpose:
#   Defines a full multi-container stack for the CoSMIC Coaching Writer system.
#   Components:
#     1. ollama          → local LLM backend for model inference
#     2. coaching-writer → FastAPI service providing RAG + coaching
#     3. open-webui      → web front-end with chat interface and pipelines
#     4. pipelines       → OpenWebUI pipeline runner, integrates custom logic
#
# Notes:
#   - The setup automatically pulls the required model (gemma2:2b).
# -----------------------------------------------------------------------------

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["/bin/sh", "/ollama-entrypoint.sh"]
    healthcheck:
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1"]
      interval: 20s
      retries: 40
    volumes:
      - ollama:/root/.ollama
      - ./scripts/ollama-entrypoint.sh:/ollama-entrypoint.sh:ro
    restart: unless-stopped
    networks:
      - cosmic_net

  coaching-writer:
    build:
      context: .
      dockerfile: Dockerfile
    image: cosmic-coaching-writer:latest
    container_name: coaching-writer
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - LLM_NAME=qwen3:4b
      - VECTOR_DB_PATH=database/vector_db
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - RETRIEVE_TOPK=5
      - TEMPERATURE=0.2
    volumes:
      - ./vector_db:/app/database/vector_db
      - ./academic-texts:/app/academic-texts
      - open-webui:/app/backend/data
    ports:
      - "8001:8001"
    networks:
      - cosmic_net
    restart: unless-stopped

  open-webui:
    build:
       context: ../OpenWebUI-CoSMIC
       dockerfile: Dockerfile
    image: opensicbr/openwebui-cosmic:standalone
    #image: opensicbr/cosmic_openwebui:latest
    container_name: openwebui
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENSI_COSMIC_API_BASE_URL=http://coaching-writer:8001
      # - OLLAMA_BASE_URLS=http://coaching-writer:8001
      - ENABLE_OLLAMA_API=true
      - ENABLE_OPENAI_API=true
      - ENABLE_OAUTH_SIGNUP=false
      - WEBUI_SECRET_KEY=dev-secret

      # --- Force your custom provider + default model ---
      - OPENAI_API_BASE_URL=http://pipelines:9099
      - OPENAI_API_KEY=0p3n-w3bu!
      - DEFAULT_MODELS=CoSMIC-CoachingWriter

      - ENABLE_PIPELINES=true
      - PIPELINES_URL=http://pipelines:9099
    volumes:
      # - ../OpenWebUI-CoSMIC:/app/src:ro
      - ./vector_db:/app/database/vector_db
      - open-webui:/app/backend/data
    depends_on:
      coaching-writer:
        condition: service_started
    ports:
      - "8080:8080"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 15s
      timeout: 10s
      retries: 10
    networks:
      - cosmic_net

  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    container_name: cw_pipelines
    environment:
      - OPENSI_COSMIC_API_BASE_URL=http://coaching-writer:8001
      # - MAX_QUERIES_PER_USER=10
      - PIPELINES_URLS=https://github.com/TheOpenSI/CoSMIC-CoachingWriter/blob/main/pipelines/coaching_writer_pipeline.py
    volumes:
      - ./pipelines:/ext_pipelines:ro
    depends_on:
      coaching-writer:
        condition: service_started
    ports:
      - "9099:9099"
    restart: unless-stopped
    networks:
      - cosmic_net

volumes:
  ollama: {}
  vector_db: {}
  volume_configs: {}
  open-webui: {}
  shared_mount:
    external: true

networks:
  cosmic_net:
    driver: bridge
